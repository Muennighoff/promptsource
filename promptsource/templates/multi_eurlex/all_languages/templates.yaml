dataset: multi_eurlex
subset: all_languages
templates:
  38ddea55-1710-4615-bbfa-fe5803e21e43: !Template
    answer_choices: null
    id: 38ddea55-1710-4615-bbfa-fe5803e21e43
    jinja: 'If the French version says: {{text["fr"]}}; then the English version
      should say:
      |||   {{text["en"]}}'
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages: []
      metrics:
      - BLEU
      original_task: true
    name: version-fr-en-source+target
    reference: ''
  2bc0e46c-d1fe-4bc9-99d1-9b61aa42cd02: !Template
    answer_choices: null
    id: 2bc0e46c-d1fe-4bc9-99d1-9b61aa42cd02
    jinja: 'If the English version says: {{text["en"]}}; then the French version
      should say:
      |||   {{text["fr"]}}'
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages: []
      metrics:
      - BLEU
      original_task: true
    name: version-en-fr-source+target
    reference: ''
  73dc1b77-e8ea-4dc8-8a12-0abc3b0dbba0: !Template
    answer_choices: null
    id: 73dc1b77-e8ea-4dc8-8a12-0abc3b0dbba0
    jinja: 'Given the following source text in French: {{text["fr"]}} , a good
      English translation is: ||| {{text["en"]}}'
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages: []
      metrics:
      - BLEU
      original_task: true
    name: a_good_translation-fr-en-source+target
  63dc1b77-e8ea-4dc8-8a12-0abc3b0dbba0: !Template
    answer_choices: null
    id: 63dc1b77-e8ea-4dc8-8a12-0abc3b0dbba0
    jinja: 'Given the following source text in English: {{text["en"]}} , a good
      French translation is: ||| {{text["fr"]}}'
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages: []
      metrics:
      - BLEU
      original_task: true
    name: a_good_translation-en-fr-source+target
  3bc0e46c-d1fe-4bc9-99d1-9b61aa42cd02: !Template
    answer_choices: null
    id: 3bc0e46c-d1fe-4bc9-99d1-9b61aa42cd02
    jinja: 'Document in English: {{text["en"]}}\n\nTranslate the previous document to proper French:
      |||  {{text["fr"]}}'
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages: []
      metrics:
      - BLEU
      original_task: true
    name: prev_doc-en-fr
    reference: ''
  5bc0e46c-d1fe-4bc9-99d1-9b61aa42cd02: !Template
    answer_choices: null
    id: 5bc0e46c-d1fe-4bc9-99d1-9b61aa42cd02
    jinja: 'Document in French: {{text["fr"]}}\n\nTranslate the previous document to proper English:
      |||  {{text["en"]}}'
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages: []
      metrics:
      - BLEU
      original_task: true
    name: prev_doc-fr-en
    reference: ''
  9bc0e46c-d1fe-4bc9-99d1-9b61aa42cd02: !Template
    answer_choices: null
    id: 9bc0e46c-d1fe-4bc9-99d1-9b61aa42cd02
    jinja: 'Document in French: {{text["fr"]}}\n\nTranslate the entire previous document to proper English sentence for sentence (min 100 words):
      |||  {{text["en"]}}'
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages: []
      metrics:
      - BLEU
      original_task: true
    name: prev_doc_long-fr-en
    reference: ''
